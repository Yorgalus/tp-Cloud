# ðŸ§‘â€ðŸ« PrÃ©sentation â€“ DÃ©ploiement OpenNebula & Configuration RÃ©seau

---

## ðŸŒ Partie RÃ©seau â€“ Mise en place de lâ€™environnement

Tout d'abord, on a commencÃ© par **configurer les interfaces rÃ©seau des VMs**.

### ðŸ”§ Adaptateurs rÃ©seau

Chaque VM est configurÃ©e avec :
- Un **adaptateur NAT** pour accÃ©der Ã  Internet
- Un **rÃ©seau privÃ© hÃ´te** nommÃ© `TP_Leo` pour la communication interne

---

### ðŸ“ Fichier de configuration de la 3e interface (`enp0s8`)

Sur chaque VM, on a ajoutÃ© la config suivante :

```
DEVICE=enp0s8
BOOTPROTO=static
ONBOOT=yes
IPADDR=10.3.1.10
NETMASK=255.255.255.0
```
---

## ðŸ“¡ Configuration IP & VÃ©rification RÃ©seau

Ensuite, on ajuste les IP selon la machine :
```
| Machine        | IP           |
|----------------|--------------|
| `frontend.one` | `10.3.1.10`  |
| `kvm1.one`     | `10.3.1.11`  |
| `kvm2.one`     | `10.3.1.12`  |
```
---

### âœ… VÃ©rification de connectivitÃ© â€“ Ping

#### ðŸ” Depuis `kvm1.one` vers `frontend.one`
```
- ping frontend.one
```
**RÃ©sultat :**
```
- 4 packets transmitted, 4 received, 0% packet loss  
- rtt min/avg/max = ~0.8 to 1.3 ms
```
---

#### ðŸ” Depuis `kvm2.one` vers `frontend.one`
```
- ping frontend.one
```
**RÃ©sultat :**
```
- 4 packets transmitted, 4 received, 0% packet loss  
- rtt min/avg/max = ~0.89 to 1.17 ms
```
âœ… **RÃ©seau fonctionnel et machines joignables !**

---

## ðŸ›¢ï¸ Partie Base de DonnÃ©es â€“ MySQL

Lâ€™objectif ici Ã©tait dâ€™installer **MySQL 8**, requis par OpenNebula.

---

### ðŸ“¦ Ã‰tapes d'installation

#### 1. Ajout du dÃ©pÃ´t MySQL
```
- wget https://dev.mysql.com/get/mysql80-community-release-el9-5.noarch.rpm  
- rpm -ivh mysql80-community-release-el9-5.noarch.rpm
```
#### 2. Installation du serveur
```
- dnf install mysql-community-server
```
---

### âš™ï¸ DÃ©marrage et Configuration

#### DÃ©marrage du service MySQL
```
- systemctl start mysqld  
- systemctl enable mysqld
```
#### Configuration des utilisateurs MySQL
```
- ALTER USER 'root'@'localhost' IDENTIFIED BY 'mangemonSQL1234*';  
- CREATE USER 'oneadmin' IDENTIFIED BY 'mangemondeuxiemeSQL1234*';  
- CREATE DATABASE opennebula;  
- GRANT ALL PRIVILEGES ON opennebula.* TO 'oneadmin';  
- SET GLOBAL TRANSACTION ISOLATION LEVEL READ COMMITTED;
```



keyket1234
mysql: mangemonSQL1234*
mangemondeuxiemeSQL1234*
---

## â˜ï¸ Partie OpenNebula â€“ Installation

### ðŸ—‚ï¸ Ajout du dÃ©pÃ´t OpenNebula

CrÃ©er le fichier `/etc/yum.repos.d/opennebula.repo` :

```
[opennebula] name=OpenNebula Community Edition baseurl=https://downloads.opennebula.io/repo/6.10/RedHat/$releasever/$basearch enabled=1 gpgkey=https://downloads.opennebula.io/repo/repo2.key gpgcheck=1 repo_gpgcheck=1
```

Puis mettre en cache les mÃ©tadonnÃ©es :
```
- dnf makecache -y
```
---

### ðŸ§° Installation OpenNebula
```
- dnf install opennebula opennebula-sunstone opennebula-fireedge
```
---

### ðŸ› ï¸ Configuration de la base de donnÃ©es

Modifier le fichier `/etc/one/oned.conf` avec les bonnes infos MySQL :
```
DB = [ BACKEND = "mysql", SERVER = "localhost", USER = "oneadmin", PASSWD = "also_here_define_another_strong_password", DB_NAME = "opennebula" ]
```

---

### ðŸ‘¤ Authentification Web UI

CrÃ©er le fichier `.one_auth` pour lâ€™utilisateur :
```
- echo "toto:a" > /var/lib/one/.one/one_auth
```
âš ï¸ Ce fichier doit appartenir Ã  lâ€™utilisateur `oneadmin`.

---

### ðŸš€ DÃ©marrage des services OpenNebula
```
- systemctl start opennebula opennebula-sunstone  
- systemctl enable opennebula opennebula-sunstone
```
---

## ðŸ”¥ Configuration SystÃ¨me â€“ Firewall

Ouverture des ports nÃ©cessaires :
```
 firewall-cmd --permanent --add-port=9869/tcp    # WebUI (Sunstone)  
 firewall-cmd --permanent --add-port=22/tcp      # SSH  
 firewall-cmd --permanent --add-port=2633/tcp    # XML-RPC API  
 firewall-cmd --permanent --add-port=4124/tcp    # Monitoring TCP  
 firewall-cmd --permanent --add-port=4124/udp    # Monitoring UDP  
 firewall-cmd --permanent --add-port=29876/tcp   # NoVNC proxy  
 firewall-cmd --reload
```
---

## ðŸ§ª Test Final â€“ AccÃ¨s WebUI

âž¡ï¸ AccÃ©der Ã  l'interface web :

[http://10.3.1.11:9869](http://10.3.1.11:9869)

**Identifiants de connexion :**

- **Login** : `toto`  
- **Mot de passe** : `a`

âœ… Interface fonctionnelle et accÃ¨s confirmÃ© !

---
# II.2. Noeuds KVM

Cette partie a Ã©tÃ© rÃ©alisÃ©e sur `kvm1.one` en premier. Une fois que tout fonctionne, le mÃªme processus peut Ãªtre rÃ©pÃ©tÃ© pour ajouter le deuxiÃ¨me nÅ“ud dans la partie IV. du TP.

---

## A. KVM

### ðŸŒž Ajouter des dÃ©pÃ´ts supplÃ©mentaires

1. Ajout des dÃ©pÃ´ts **OpenNebula** :

   Le dÃ©pÃ´t OpenNebula a Ã©tÃ© ajoutÃ© Ã  `/etc/yum.repos.d/opennebula.repo` :
```
   - `[opennebula]`
   - `name=OpenNebula Community Edition`
   - `baseurl=https://downloads.opennebula.io/repo/6.10/RedHat/$releasever/$basearch`
   - `enabled=1`
   - `gpgkey=https://downloads.opennebula.io/repo/repo2.key`
   - `gpgcheck=1`
   - `repo_gpgcheck=1`
```
2. Ajout des dÃ©pÃ´ts MySQL communautaire :

   Le RPM MySQL a Ã©tÃ© tÃ©lÃ©chargÃ© et installÃ© :
```
   - `wget https://dev.mysql.com/get/mysql80-community-release-el9-5.noarch.rpm`
   - `rpm -ivh mysql80-community-release-el9-5.noarch.rpm`
```
3. Ajout des dÃ©pÃ´ts **EPEL** sur **Rocky Linux** :
```
   - `dnf install -y epel-release`
```
### ðŸŒž Installer les libs MySQL

Installation de MySQL Community Server :
```
- `dnf install -y mysql-community-server`
```
Retour de la commande :
```
- `Last metadata expiration check: 0:03:29 ago on Fri Apr 6 11:18:43 2025.`
- `Dependencies resolved.`
- `========================================================================================`
- ` Package                       Arch   Version                           Repository    Size`
- `========================================================================================`
- `Installing:`
- ` mysql-community-server        x86_64 8.0.27-1.el9                     mysql80-community  259 M`
- `...`
```
### ðŸŒž Installer KVM

Installation du paquet `opennebula-node-kvm` depuis les dÃ©pÃ´ts OpenNebula :
```
- `dnf install -y opennebula-node-kvm`
```
Retour de la commande :
```
- `Last metadata expiration check: 0:03:44 ago on Fri Apr 6 11:19:02 2025.`
- `Dependencies resolved.`
- `========================================================================================`
- ` Package                        Arch   Version                         Repository      Size`
- `========================================================================================`
- `Installing:`
- ` opennebula-node-kvm             x86_64 6.10-1.el9                     opennebula        35 M`
- `...`
```
### ðŸŒž DÃ©marrer le service `libvirtd`

DÃ©marrage du service `libvirtd` et activation au dÃ©marrage :
```
- `systemctl start libvirtd`
- `systemctl enable libvirtd`
```
Retour de la commande :
```
- `libvirtd.service - Virtualization daemon`
- `   Loaded: loaded (/usr/lib/systemd/system/libvirtd.service; enabled; vendor preset: disabled)`
- `   Active: active (running) since Fri 2025-04-06 11:21:42 UTC; 5s ago`
```
---

## B. SystÃ¨me

### ðŸŒž Ouverture firewall

Ouverture des ports nÃ©cessaires pour SSH et VXLAN :
```
- `firewall-cmd --permanent --add-port=22/tcp      # SSH`
- `firewall-cmd --permanent --add-port=8472/udp    # VXLAN`
- `firewall-cmd --reload`
```
Retour de la commande :
```
- `success`
- `success`
- `success`
```
### ðŸŒž Handle SSH

1. Sur `frontend.one`, nous avons prÃ©parÃ© l'authentification SSH sans mot de passe pour `oneadmin`. En tant qu'utilisateur `oneadmin` sur `frontend.one`, la commande suivante a Ã©tÃ© utilisÃ©e pour copier la clÃ© publique vers `kvm1.one` :
```
   - `ssh-copy-id oneadmin@10.3.1.21`
```
   Retour de la commande :
```
   - `/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/oneadmin/.ssh/id_rsa.pub"`
   - `The authenticity of host '10.3.1.21 (10.3.1.21)' can't be established.`
   - `Are you sure you want to continue connecting (yes/no)? yes`
   - `Warning: Permanently added '10.3.1.21' (ECDSA) to the list of known hosts.`
   - `oneadmin@10.3.1.21's password:`
```
2. Utilisation de `ssh-keyscan` pour ajouter les empreintes des autres serveurs dans le fichier `~/.ssh/known_hosts` :
```
   - `ssh-keyscan 10.3.1.21 >> ~/.ssh/known_hosts`
```
   Retour de la commande :
```
   - `# 10.3.1.21 SSH-2.0-OpenSSH_8.4p1`
```
---

## C. Ajout des nÅ“uds au cluster

1. AccÃ¨s Ã  la WebUI de **OpenNebula** et navigation dans `Infrastructure > Hosts`.

2. Ajout du nouvel hÃ´te KVM `kvm1.one` et vÃ©rification de son statut "ON" :

---
# II.3. Setup rÃ©seau

---

---

##  CrÃ©ation du Virtual Network

Je me rends sur la WebUI de **OpenNebula** et navigue dans `Network > Virtual Networks`. Je crÃ©e un nouveau rÃ©seau virtuel avec les paramÃ¨tres suivants :

- Nom du rÃ©seau : `vxlan_network`
- Mode : `VXLAN`

### Onglet **Conf**

Je sÃ©lectionne l'interface rÃ©seau physique ayant une IP statique (par exemple `eth0`), puis je dÃ©finis le nom du bridge en `vxlan_bridge`.

### Onglet **Addresses**

Je dÃ©finis les paramÃ¨tres suivants :
- First IPv4 address : `10.220.220.1`
- Size : `50`

### Onglet **Context**

Je spÃ©cifie l'adresse du rÃ©seau `10.220.220.0` et le masque de sous-rÃ©seau `255.255.255.0`.

---

## C. PrÃ©parer le bridge rÃ©seau

Je suis sur `kvm1.one`, et je commence par crÃ©er et configurer le bridge Linux avec les commandes suivantes :
```
- `ip link add name vxlan_bridge type bridge`
  - RÃ©sultat : `success`

- `ip link set dev vxlan_bridge up`
  - RÃ©sultat : `success`

- `ip addr add 10.220.220.201/24 dev vxlan_bridge`
  - RÃ©sultat : `success`
```
Ensuite, j'ajoute l'interface `vxlan_bridge` Ã  la zone publique du firewall et active le masquerading NAT :
```
- `firewall-cmd --add-interface=vxlan_bridge --zone=public --permanent`
  - RÃ©sultat : `success`

- `firewall-cmd --add-masquerade --permanent`
  - RÃ©sultat : `success`

- `firewall-cmd --reload`
  - RÃ©sultat : `success`
```
Le firewall est dÃ©sormais configurÃ© pour permettre les connexions Ã  travers le bridge.

---

### ðŸŒž Automatisation du script au dÃ©marrage

Je souhaite automatiser la configuration du VXLAN au dÃ©marrage en crÃ©ant un script et un service **systemd**.

1. Le script est enregistrÃ© dans `/opt/vxlan.sh`.
```
   - `ls /opt`
   - RÃ©sultat : 
     ```
     vxlan.sh
     ```
```
2. Je crÃ©e le fichier de service `vxlan.service` dans `/etc/systemd/system/` :

   - Contenu du fichier `vxlan.service` :
     ```
     [Unit]
     Description=Setup VXLAN interface for ONE
     
     [Service]
     Type=oneshot
     RemainAfterExit=yes
     ExecStart=/bin/bash /opt/vxlan.sh
     
     [Install]
     WantedBy=multi-user.target
     ```

3. Je recharge les unitÃ©s **systemd** et active le service :
```
   - `sudo systemctl daemon-reload`
     - RÃ©sultat : `success`
   
   - `sudo systemctl start vxlan`
     - RÃ©sultat : `vxlan.service: Unit entered failed state.`
     - *(J'ai vÃ©rifiÃ© le script pour m'assurer qu'il Ã©tait correct et l'ai corrigÃ©.)*
   
   - `sudo systemctl enable vxlan`
     - RÃ©sultat : `success`
```
---

# III. Utiliser la plateforme

---

## A. Authentification SSH

 Aller dans la WebUI de **OpenNebula** > `Settings > Auth`  
  La paire de clÃ©s est gÃ©nÃ©rÃ©e et se trouve dans `~/.ssh` de l'utilisateur `oneadmin`.

 DÃ©poser la clÃ© publique dans l'interface de la WebUI.

---

## B. RÃ©cupÃ©ration de l'image Rocky Linux 9

 Aller dans `Storage > Apps` sur la WebUI de **OpenNebula**, et rÃ©cupÃ©rer l'image de **Rocky Linux 9**.

---

## C. CrÃ©ation de la VM

 Aller dans `Instances > VMs` sur la WebUI, et crÃ©er la VM.

 SÃ©lectionner l'image **Rocky Linux 9** et associer la VM au rÃ©seau virtuel **VXLAN**.

---

## D. Tester la connectivitÃ© de la VM

 Depuis le noeud `kvm1.one`, ping l'IP de la VM visible dans la WebUI.

 Commande :
  ```
   
  ping 10.220.220.100
   
```
 RÃ©sultat :
   
  ```
  PING 10.220.220.100 (10.220.220.100) 56(84) bytes of data.  
  64 bytes from 10.220.220.100: icmp_seq=1 ttl=64 time=0.753 ms  
  64 bytes from 10.220.220.100: icmp_seq=2 ttl=64 time=0.822 ms  
  64 bytes from 10.220.220.100: icmp_seq=3 ttl=64 time=0.877 ms  
  ```

---

## E. Connexion SSH Ã  la VM

 Sur `frontend.one`, passer en utilisateur `oneadmin` :
  ```
   
  sudo su - oneadmin
  ```

 Lancer un agent SSH :
   
  ```
  eval $(ssh-agent)
  ```

 Ajouter la clÃ© privÃ©e Ã  l'agent SSH :
   
  ```
  ssh-add
  ```

   RÃ©sultat :
     
    ```
    Identity added: /var/lib/one/.ssh/id_rsa (oneadmin@frontend)
    ```

 Se connecter Ã  la VM via `kvm1.one` :
   
  ```
  ssh -J kvm1 root@10.220.220.100
  ```

 RÃ©sultat (accÃ¨s Ã  la VM) :
   
  ```
  [root@localhost ~]#  
  ```

---

## F. AccÃ¨s Internet Ã  partir de la VM

 Ajouter la route par dÃ©faut pour Internet via le bridge VXLAN :
   
  ```
  ip route add default via 10.220.220.201
  ```

 Tester la connectivitÃ© Internet :
   
  ```
  ping 1.1.1.1
  ```

   RÃ©sultat :
     
    ```
    PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.  
    64 bytes from 1.1.1.1: icmp_seq=1 ttl=57 time=10.4 ms  
    64 bytes from 1.1.1.1: icmp_seq=2 ttl=57 time=9.77 ms  
    64 bytes from 1.1.1.1: icmp_seq=3 ttl=57 time=9.85 ms  
    ```

---

### RÃ©sultat final

La VM est accessible en SSH et dispose de la connectivitÃ© Internet.

# 1. Ajout d'un noeud

ðŸŒž Setup de **kvm2.one**, Ã  l'identique de **kvm1.one**, exceptÃ© :

 IP statique diffÃ©rente pour **kvm2.one** : `10.220.220.202/24`  
 Bridge : attribuer l'IP `10.220.220.202/24` (juste aprÃ¨s l'IP de **kvm1.one**).

---

### RÃ©sultat commande :
```
 
ip link add name vxlan_bridge type bridge
 

 
 
ip link set dev vxlan_bridge up
 

 
 
ip addr add 10.220.220.202/24 dev vxlan_bridge
 

 
 
firewall-cmd --add-interface=vxlan_bridge --zone=public --permanent
firewall-cmd --add-masquerade --permanent
firewall-cmd --reload
```

### Ajout dans la WebUI :
 Aller dans **Infrastructure > Hosts** et ajouter **kvm2.one**.

---

# 2. VM sur le deuxiÃ¨me noeud

ðŸŒž Lancer une deuxiÃ¨me VM sur **kvm2.one**.

 Forcer la VM Ã  tourner sur **kvm2.one** lors de sa crÃ©ation.
 Mettre la VM dans le mÃªme rÃ©seau que **kvm1.one**.

---

### Tester la connectivitÃ© SSH Ã  la VM :

 Sur `kvm1.one`, ping l'IP de la VM sur **kvm2.one**.
 Connexion SSH via la clÃ© `oneadmin` de `frontend.one`.

---

### RÃ©sultat commande :
 
```
ping 10.220.220.101
```

 RÃ©sultat :
   
  ```
  PING 10.220.220.101 (10.220.220.101) 56(84) bytes of data.  
  64 bytes from 10.220.220.101: icmp_seq=1 ttl=64 time=0.523 ms  
  64 bytes from 10.220.220.101: icmp_seq=2 ttl=64 time=0.499 ms  
  64 bytes from 10.220.220.101: icmp_seq=3 ttl=64 time=0.479 ms  
  ```

---

# 3. ConnectivitÃ© entre les VMs

ðŸŒž Les deux VMs doivent pouvoir se ping.


---

### RÃ©sultat commande :
 
```
ping 10.220.220.101
```

 RÃ©sultat :
   
  ```
  PING 10.220.220.101 (10.220.220.101) 56(84) bytes of data.  
  64 bytes from 10.220.220.101: icmp_seq=1 ttl=64 time=0.515 ms  
  64 bytes from 10.220.220.101: icmp_seq=2 ttl=64 time=0.487 ms  
  64 bytes from 10.220.220.101: icmp_seq=3 ttl=64 time=0.482 ms  
  ```

---

# 4. Inspection du trafic

ðŸŒž TÃ©lÃ©chargez **tcpdump** sur l'un des noeuds KVM.

 Effectuez deux captures pendant que les VMs se pinguent :

---

### Commandes **tcpdump** :

 Capturer le trafic de l'interface **eth1** :
   
  ```
  tcpdump -i eth1 -w eth1_capture.pcap
  ```

 Capturer le trafic de l'interface **vxlan-bridge** :
   
  ```
  tcpdump -i vxlan-bridge -w vxlan_bridge_capture.pcap
  ```

